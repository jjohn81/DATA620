{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joby John Week 8 \n",
    "\n",
    "\n",
    "\n",
    "### Video Presentation\n",
    "\n",
    "\n",
    "\n",
    "### Assignment\n",
    "\n",
    "In this week's assignment, you are asked to analyze high frequency words.\n",
    "Please answer the fo llowing questions in an Jupyter Notebook, posted to GitHub.\n",
    "1.\tChoose a corpus of interest.\n",
    "2.\tHow many total unique words are in the corpus?  (Please feel free to define unique words in any interesting, defensible way).\n",
    "3.\tTaking the most common words, how many unique words represent half of the total words in the corpus?Identify the 200 highest frequency words in this corpus.\n",
    "4.\tCreate a graph that shows the relative frequency of these 200 words.\n",
    "5.\tDoes the observed relative frequency of these words follow Zipf’s law? Explain.\n",
    "6.\tIn what ways do you think the frequency of the words in this corpus differ from “all words in all corpora.”\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Choose a corpus of interest.\n",
    "\n",
    "The text 'melville-moby_dick.txt' contains <b>260819</b> words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moby = gutenberg.words('melville-moby_dick.txt')\n",
    " \n",
    "len(moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are <b>19317</b> unique words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(set(moby))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) How many total unique words are in the corpus?  \n",
    "(Please feel free to define unique words in any interesting, defensible way)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will define unique word as : anything that is more than 2 alphabetic characters long and that is not a stop word. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of words :  260819\n",
      "Total Number of alphabetic words :  218361\n",
      "Total Number of words after removing stopwords :  110459\n",
      "Total Number of words with 2 or more characters(unique) :  16742\n",
      "Total Number of words with 2 or less characters :  60\n"
     ]
    }
   ],
   "source": [
    "print('Total Number of words :  {}'.format(len(moby)))\n",
    "# Stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    " \n",
    "# keep only alphabetic characters\n",
    "alpha_words = [word.lower() for word in moby if word.isalpha()]\n",
    "\n",
    "\n",
    "print('Total Number of alphabetic words :  {}'.format(len(alpha_words)))\n",
    "\n",
    "# Remove stopwords\n",
    "words = [w for w in alpha_words if w not in stop_words]\n",
    "\n",
    " \n",
    "\n",
    "print('Total Number of words after removing stopwords :  {}'.format(len(words)))\n",
    "\n",
    " \n",
    "unique_finalWords = set([word for word in words if len(word)>2])\n",
    "\n",
    "print('Total Number of words with 2 or more characters(unique) :  {}'.format(len(unique_finalWords)))\n",
    "lessthanThreewords = set([word for word in words if len(word)<=2])\n",
    "print('Total Number of words with 2 or less characters :  {}'.format(len(lessthanThreewords)))\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below table shows words that are two characters long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c', 'ah', 'ee', 'si', 'em', 'lo', 'dr', 'ex', 'da', 'v', 'ho', 'ox', 'ii', 'ka', 'e', 'ge', 'ob', 'f', 'co', 'lt', 'mt', 'os', 'g', 'j', 'bo', 'n', 'en', 'mr', 'u', 'eh', 'se', 'et', 'ye', 'le', 'vi', 'de', 'st', 'fa', 'oh', 'sw', 'iv', 'wa', 'x', 'us', 'ay', 'fe', 'te', 'io', 'p', 'ha', 'go', 'l', 'nt', 'w', 'er', 'la', 'tu', 'ne', 'h', 'um'}\n"
     ]
    }
   ],
   "source": [
    "print(lessthanThreewords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 16742  unique words in the Moby-Dick text after removing stopwords and two character words. Originally, the text had 19317 unique words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Taking the most common words, how many unique words represent half of the total words in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'whale': 1226, 'one': 921, 'like': 647, 'upon': 566, 'man': 527, 'ship': 518, 'ahab': 511, 'ye': 472, 'sea': 455, 'old': 450, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(words)\n",
    "fdist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 110459 words (non unique). About half of this is 55229.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110459\n",
      "55229.5\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(len(words)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>cumulative_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>whale</td>\n",
       "      <td>1226</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "      <td>921</td>\n",
       "      <td>2147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like</td>\n",
       "      <td>647</td>\n",
       "      <td>2794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>upon</td>\n",
       "      <td>566</td>\n",
       "      <td>3360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>man</td>\n",
       "      <td>527</td>\n",
       "      <td>3887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  frequency  cumulative_frequency\n",
       "0  whale       1226                  1226\n",
       "1    one        921                  2147\n",
       "2   like        647                  2794\n",
       "3   upon        566                  3360\n",
       "4    man        527                  3887"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Cumulative Frequency Table\n",
    "df_all_words = pd.DataFrame(words)\n",
    "df_all_words.columns = ['word']\n",
    "df_all_words_freq = pd.DataFrame(df_all_words['word'].value_counts()).reset_index(drop=False)\n",
    "df_all_words_freq.columns = ['word', 'frequency']\n",
    "df_all_words_freq['cumulative_frequency'] = df_all_words_freq['frequency'].cumsum()\n",
    "df_all_words_freq.sort_values(by='frequency', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 258 words that represent about half of the total words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all_words_freq[df_all_words_freq['cumulative_frequency'] <= 130409])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half = df_all_words_freq[df_all_words_freq['cumulative_frequency'] <= 130409.5]\n",
    "print(len(half))\n",
    "half.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(3,figsize=(25,10)) \n",
    "#%matplotlib inline\n",
    "moby_freq_dist.plot(258, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Identify the 200 highest frequency words in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words = fdist.most_common(200)\n",
    "\n",
    "\n",
    "top_wordsdf = pd.DataFrame(top_words, columns = ['Word', 'Freq'])\n",
    "\n",
    "#Calculate Relative_Frequency\n",
    "top_wordsdf['Relative_Freq'] = top_wordsdf['Freq']/top_wordsdf['Freq'].sum()\n",
    "top_wordsdf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Create a graph that shows the relative frequency of these 200 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 12))\n",
    "plt.rc('xtick', labelsize=16) \n",
    "fdist.plot(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Does the observed relative frequency of these words follow Zipf’s law? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the English language, the probability of encountering the rth most common word is given roughly by P(r)=0.1/r. \n",
    "\n",
    "Based on the plot below, we see that 200 most common words roughly follow Zipf's law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zip_probability'] = 0.1/(df['index'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3,figsize=(20,10))\n",
    "plt.plot(df['relative_frequency'], 'blue')\n",
    "plt.plot(df['zip_probability'], 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) In what ways do you think the frequency of the words in this corpus differ from “all words in all corpora.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of the words in a specific text could differ from the frequency of \"all words in all corpora\" based on the author and the subject. Some authors might use a set of words more frequently than other authors. Another way would be the by topic/subject. For example, this text uses the word 'Whale' a lot but a book about physiscs or math probably will never the term 'Whale'.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
