{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DATA 620 - Project 3</h2>\n",
    "<h3>Name Gender Classifier</h3>\n",
    "\n",
    "<h3>Joby John</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment</h3>\n",
    "\n",
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can.\n",
    "\n",
    "We will split dataset into three subsets. 80% data for training, 10 % data for dev-test and 10 %  for testing. We are mainting to the seperate test and dev-test. \n",
    "\n",
    "We will compare the performance of different classifiers on these datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk.classify import apply_features\n",
    "import collections\n",
    "import nltk.metrics\n",
    "from nltk.metrics import precision,recall,f_measure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data</h3>\n",
    "\n",
    "We will import the male and female ness from NLTk librarby and shuffle the names so the sampling is random acorss all the names.  'male.txt' contains 2943 names and 'female.txt' contains 5001 names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2943\n",
      "5001\n"
     ]
    }
   ],
   "source": [
    "random.seed(100)\n",
    "\n",
    "print(len(names.words('male.txt')))\n",
    "print(len(names.words('female.txt')))\n",
    "      \n",
    "names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "[(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "# and to make sure we are sampling across all the names\n",
    "# we shuffle them so they aren't alphabetical\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sample Data</h4>\n",
    "Belows we print some of the names in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pembroke', 'male'), ('Bryant', 'male'), ('Pier', 'female'), ('Florice', 'female'), ('Clint', 'male'), ('Ruthanne', 'female'), ('Upton', 'male'), ('Yetty', 'female'), ('Silvie', 'female'), ('Lizzie', 'female')]\n"
     ]
    }
   ],
   "source": [
    "#list names\n",
    " \n",
    "print(names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Split dataset : Train, Dev-Test and Test </h4>\n",
    "We will 10%(800 names) data fort Test and Dev-Test. 80%(6344) the data will be used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Names :  7944\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Names : \" , len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Count :  800\n",
      "dev test set :  800\n",
      "train set:  6344\n"
     ]
    }
   ],
   "source": [
    "test_set = names[:800]\n",
    "print(\"Test Set Count : \" , len(test_set))\n",
    "devtest_set = names[800:1600] \n",
    "print(\"dev test set : \" , len(devtest_set))\n",
    "train_set = names[1600:]\n",
    "print(\"train set: \" , len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Set - Gender</h3>\n",
    "We will define the features set as the first letter, last letter, last two letters, first two letters, number of vowles and has letter in the vowel. \n",
    "We will use Navie Bayes and Decision Trees and compare their performance to see which is good at predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(name):\n",
    "    f = {}\n",
    "    f[\"first\"] = name[0].lower()\n",
    "    f[\"last\"] = name[-1].lower()\n",
    "    f[\"suffix2\"]= name[-2:].lower()\n",
    "    f[\"preffix2\"]= name[:2].lower()\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features1(name):\n",
    "    f = {}\n",
    "    f[\"first\"] = name[0].lower()\n",
    "    f[\"last\"] = name[-1].lower()\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = [(features(n), g) for (n,g) in train_set]\n",
    "test_f = [(features(n), g) for (n,g) in test_set]\n",
    "devtest_f = [(features(n), g) for (n,g) in devtest_set]\n",
    "\n",
    "train_f1 = [(features1(n), g) for (n,g) in train_set]\n",
    "test_f1 = [(features1(n), g) for (n,g) in test_set]\n",
    "devtest_f1 = [(features1(n), g) for (n,g) in devtest_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Naive Bayes </h3>\n",
    "We will use Naive Bayes classifier and calculate Accuracy, Recall, Precision and F_Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nbclassifier = nltk.NaiveBayesClassifier.train(train_f)\n",
    "nbclassifier1 = nltk.NaiveBayesClassifier.train(train_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate Metrics: Accuracy, Recall, Precision, F_Measure </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(cls, dataset ):\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "\n",
    "    for i, (f, label) in enumerate(dataset):\n",
    "        refsets[label].add(i)\n",
    "        observed = cls.classify(f)\n",
    "        testsets[observed].add(i)\n",
    "\n",
    "    a = nltk.classify.accuracy(cls, dataset)\n",
    "    p = (precision(refsets['male'], testsets['male'])+precision(refsets['male'], testsets['female']))/2\n",
    "    r = (recall(refsets['male'], testsets['male'])+recall(refsets['male'], testsets['female']))/2\n",
    "    f = (f_measure(refsets['male'], testsets['male'])+f_measure(refsets['male'], testsets['female']))/2\n",
    "\n",
    "    return (a, p,r,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracy_train, avg_precision_train, avg_recall_train, avg_f_measure_train = getMetrics(nbclassifier, train_f)\n",
    " \n",
    "accuracy_test ,  avg_precision_test ,avg_recall_test , avg_f_measure_test = getMetrics(nbclassifier, test_f)\n",
    "\n",
    "accuracy_dev ,  avg_precision_dev ,avg_recall_dev , avg_f_measure_dev = getMetrics(nbclassifier, devtest_f)\n",
    "\n",
    " \n",
    "accuracy_naive_final = [accuracy_train,accuracy_test,accuracy_dev]\n",
    "precision_naive_final = [avg_precision_train,avg_precision_test,avg_precision_dev]\n",
    "recall_naive_final = [avg_recall_train,avg_recall_test,avg_recall_dev]\n",
    "f_measure_naive_final = [avg_f_measure_train,avg_f_measure_test,avg_f_measure_dev]\n",
    "\n",
    "naive_final_train = [accuracy_train,avg_precision_train,avg_recall_train,avg_f_measure_train]\n",
    "naive_final_test = [accuracy_test,avg_precision_test,avg_recall_test,avg_f_measure_test]\n",
    "naive_final_dev = [accuracy_dev,avg_precision_dev,avg_recall_dev,avg_f_measure_dev]\n",
    "naive_combined = {'Accuracy':accuracy_naive_final,'Precision':precision_naive_final,'Recall':recall_naive_final,'F_Measure':f_measure_naive_final}\n",
    "df_naive = pd.DataFrame(naive_combined,index=['Train','Test','Dev'],columns=['Accuracy','Precision','Recall','F_Measure'])\n",
    "\n",
    "\n",
    "\n",
    "accuracy_train, avg_precision_train, avg_recall_train, avg_f_measure_train = getMetrics(nbclassifier1, train_f1)\n",
    " \n",
    "accuracy_test ,  avg_precision_test ,avg_recall_test , avg_f_measure_test = getMetrics(nbclassifier1, test_f1)\n",
    "\n",
    "accuracy_dev ,  avg_precision_dev ,avg_recall_dev , avg_f_measure_dev = getMetrics(nbclassifier1, devtest_f1)\n",
    "\n",
    " \n",
    "accuracy_naive_final1 = [accuracy_train,accuracy_test,accuracy_dev]\n",
    "precision_naive_final1 = [avg_precision_train,avg_precision_test,avg_precision_dev]\n",
    "recall_naive_final1 = [avg_recall_train,avg_recall_test,avg_recall_dev]\n",
    "f_measure_naive_final1 = [avg_f_measure_train,avg_f_measure_test,avg_f_measure_dev]\n",
    "\n",
    "naive_final_train1 = [accuracy_train,avg_precision_train,avg_recall_train,avg_f_measure_train]\n",
    "naive_final_test1 = [accuracy_test,avg_precision_test,avg_recall_test,avg_f_measure_test]\n",
    "naive_final_dev1 = [accuracy_dev,avg_precision_dev,avg_recall_dev,avg_f_measure_dev]\n",
    "naive_combined1 = {'Accuracy':accuracy_naive_final1,'Precision':precision_naive_final1,'Recall':recall_naive_final1,'F_Measure':f_measure_naive_final1}\n",
    "df_naive1 = pd.DataFrame(naive_combined1,index=['Train','Test','Dev'],columns=['Accuracy','Precision','Recall','F_Measure'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the metrics below we see feature set 1 has better accuracy acoss all different datasets. This makes sense as the featureset 1 contains 4 features where as feature set 2 contians only 2 fetaures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features set 1 : \n",
      "       Accuracy  Precision  Recall  F_Measure\n",
      "Train  0.821721   0.446117     0.5   0.466987\n",
      "Test   0.802500   0.421146     0.5   0.453050\n",
      "Dev    0.802500   0.456967     0.5   0.473553\n",
      "Features set 2: \n",
      "       Accuracy  Precision  Recall  F_Measure\n",
      "Train  0.779792   0.462746     0.5   0.468246\n",
      "Test   0.765000   0.438399     0.5   0.454943\n",
      "Dev    0.772500   0.473111     0.5   0.476667\n"
     ]
    }
   ],
   "source": [
    "print('Features set 1 : ')\n",
    "print(df_naive)\n",
    "print('Features set 2: ')\n",
    "print(df_naive1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Naive Baiyer features </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 suffix2 = 'na'           female : male   =     85.3 : 1.0\n",
      "                 suffix2 = 'ia'           female : male   =     79.8 : 1.0\n",
      "                 suffix2 = 'rt'             male : female =     51.7 : 1.0\n",
      "                    last = 'a'            female : male   =     34.3 : 1.0\n",
      "                 suffix2 = 'sa'           female : male   =     32.2 : 1.0\n",
      "                 suffix2 = 'ra'           female : male   =     31.1 : 1.0\n",
      "                 suffix2 = 'ld'             male : female =     30.6 : 1.0\n",
      "                    last = 'k'              male : female =     27.4 : 1.0\n",
      "                 suffix2 = 'do'             male : female =     25.0 : 1.0\n",
      "                 suffix2 = 'us'             male : female =     25.0 : 1.0\n",
      "                 suffix2 = 'ta'           female : male   =     22.5 : 1.0\n",
      "                 suffix2 = 'rd'             male : female =     20.2 : 1.0\n",
      "                 suffix2 = 'io'             male : female =     16.3 : 1.0\n",
      "                 suffix2 = 'im'             male : female =     16.1 : 1.0\n",
      "                    last = 'f'              male : female =     14.6 : 1.0\n",
      "                    last = 'v'              male : female =     14.2 : 1.0\n",
      "                 suffix2 = 'ch'             male : female =     13.0 : 1.0\n",
      "                preffix2 = 'fo'             male : female =     12.8 : 1.0\n",
      "                 suffix2 = 'ea'           female : male   =     11.8 : 1.0\n",
      "                 suffix2 = 'lo'             male : female =     11.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nbclassifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate erorr rate</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check errors\n",
    "errors = []\n",
    "for (name, tag) in devtest_set:\n",
    "    guess = nbclassifier.classify(features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=female   guess=male     name=Adelind                       \n",
      "correct=female   guess=male     name=Alix                          \n",
      "correct=female   guess=male     name=Anais                         \n",
      "correct=female   guess=male     name=Arabel                        \n",
      "correct=female   guess=male     name=Ardelis                       \n",
      "correct=female   guess=male     name=Arleen                        \n",
      "correct=female   guess=male     name=Astrix                        \n",
      "correct=female   guess=male     name=Austin                        \n",
      "correct=female   guess=male     name=Avrit                         \n",
      "correct=female   guess=male     name=Barb                          \n",
      "correct=female   guess=male     name=Beau                          \n",
      "correct=female   guess=male     name=Bel                           \n",
      "correct=female   guess=male     name=Berry                         \n",
      "correct=female   guess=male     name=Bill                          \n",
      "correct=female   guess=male     name=Birgit                        \n",
      "correct=female   guess=male     name=Brandais                      \n",
      "correct=female   guess=male     name=Brandy                        \n",
      "correct=female   guess=male     name=Bridgett                      \n",
      "correct=female   guess=male     name=Britney                       \n",
      "correct=female   guess=male     name=Charmion                      \n"
     ]
    }
   ],
   "source": [
    "for (tag, guess, name) in sorted(errors)[:20]:\n",
    "    print('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Error Count - NB</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error count:  158\n"
     ]
    }
   ],
   "source": [
    "naive_errors = len(errors)\n",
    "print(\"Error count: \", naive_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decision Tree</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Decision Tree classifier and calculate Accuracy, Recall, Precision and F_Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtclassifier = nltk.DecisionTreeClassifier.train(train_f)\n",
    "dtclassifier1 = nltk.DecisionTreeClassifier.train(train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate Metrics</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train_dt, avg_precision_train_dt, avg_recall_train_dt , avg_f_measure_train_dt = getMetrics(dtclassifier, train_f)\n",
    " \n",
    "accuracy_test_dt, avg_precision_test_dt, avg_recall_test_dt , avg_f_measure_test_dt = getMetrics(dtclassifier, test_f)\n",
    "\n",
    "accuracy_dev_dt, avg_precision_dev_dt, avg_recall_dev_dt , avg_f_measure_dev_dt = getMetrics(dtclassifier, devtest_f)\n",
    "\n",
    " \n",
    "accuracy_dt_final = [accuracy_train_dt,accuracy_test_dt,accuracy_dev_dt]\n",
    "precision_dt_final = [avg_precision_train_dt,avg_precision_test_dt,avg_precision_dev_dt]\n",
    "recall_dt_final = [avg_recall_train_dt,avg_recall_test_dt,avg_recall_dev_dt]\n",
    "f_measure_dt_final = [avg_f_measure_train_dt,avg_f_measure_test_dt,avg_f_measure_dev_dt]\n",
    "\n",
    "dt_final_train = [accuracy_train_dt,avg_precision_train_dt,avg_recall_train_dt,avg_f_measure_train_dt]\n",
    "dt_final_test = [accuracy_test_dt,avg_precision_test_dt,avg_recall_test_dt,avg_f_measure_test_dt]\n",
    "dt_final_dev = [accuracy_dev_dt,avg_precision_dev_dt,avg_recall_dev_dt,avg_f_measure_dev_dt]\n",
    "dt_combined = {'Accuracy':accuracy_dt_final,'Precision':precision_dt_final,'Recall':recall_dt_final,'F_Measure':f_measure_dt_final}\n",
    "df_dt = pd.DataFrame(dt_combined,index=['Train','Test','Dev'],columns=['Accuracy','Precision','Recall','F_Measure'])\\\n",
    "\n",
    "\n",
    "accuracy_train_dt, avg_precision_train_dt, avg_recall_train_dt , avg_f_measure_train_dt = getMetrics(dtclassifier1, train_f1)\n",
    " \n",
    "accuracy_test_dt, avg_precision_test_dt, avg_recall_test_dt , avg_f_measure_test_dt = getMetrics(dtclassifier1, test_f1)\n",
    "\n",
    "accuracy_dev_dt, avg_precision_dev_dt, avg_recall_dev_dt , avg_f_measure_dev_dt = getMetrics(dtclassifier1, devtest_f1)\n",
    "\n",
    " \n",
    "accuracy_dt_final1 = [accuracy_train_dt,accuracy_test_dt,accuracy_dev_dt]\n",
    "precision_dt_final1 = [avg_precision_train_dt,avg_precision_test_dt,avg_precision_dev_dt]\n",
    "recall_dt_final1 = [avg_recall_train_dt,avg_recall_test_dt,avg_recall_dev_dt]\n",
    "f_measure_dt_final1 = [avg_f_measure_train_dt,avg_f_measure_test_dt,avg_f_measure_dev_dt]\n",
    "\n",
    "dt_final_train1 = [accuracy_train_dt,avg_precision_train_dt,avg_recall_train_dt,avg_f_measure_train_dt]\n",
    "dt_final_test1 = [accuracy_test_dt,avg_precision_test_dt,avg_recall_test_dt,avg_f_measure_test_dt]\n",
    "dt_final_dev1 = [accuracy_dev_dt,avg_precision_dev_dt,avg_recall_dev_dt,avg_f_measure_dev_dt]\n",
    "dt_combined1 = {'Accuracy':accuracy_dt_final1,'Precision':precision_dt_final1,'Recall':recall_dt_final1,'F_Measure':f_measure_dt_final1}\n",
    "df_dt1 = pd.DataFrame(dt_combined1,index=['Train','Test','Dev'],columns=['Accuracy','Precision','Recall','F_Measure'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the metrics below we see feature set 1 has better accuracy acoss all different datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features set 1 : \n",
      "       Accuracy  Precision  Recall  F_Measure\n",
      "Train  0.918821   0.494332     0.5   0.492855\n",
      "Test   0.770000   0.430172     0.5   0.453580\n",
      "Dev    0.766250   0.471696     0.5   0.475605\n",
      "Features set 2 : \n",
      "       Accuracy  Precision  Recall  F_Measure\n",
      "Train  0.803279   0.466522     0.5   0.472399\n",
      "Test   0.767500   0.435571     0.5   0.454684\n",
      "Dev    0.767500   0.468688     0.5   0.474969\n"
     ]
    }
   ],
   "source": [
    "print('Features set 1 : ')\n",
    "print(df_dt)\n",
    "print('Features set 2 : ')\n",
    "print(df_dt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate error rate </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors2 = []\n",
    "for (name, tag) in devtest_set:\n",
    "    guess = dtclassifier.classify(features(name))\n",
    "    if guess != tag:\n",
    "        errors2.append( (tag, guess, name) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=female   guess=male     name=Adelind                       \n",
      "correct=female   guess=male     name=Alix                          \n",
      "correct=female   guess=male     name=Anais                         \n",
      "correct=female   guess=male     name=Arabel                        \n",
      "correct=female   guess=male     name=Ardelis                       \n",
      "correct=female   guess=male     name=Arleen                        \n",
      "correct=female   guess=male     name=Astrix                        \n",
      "correct=female   guess=male     name=Austin                        \n",
      "correct=female   guess=male     name=Avrit                         \n",
      "correct=female   guess=male     name=Barb                          \n",
      "correct=female   guess=male     name=Beau                          \n",
      "correct=female   guess=male     name=Bel                           \n",
      "correct=female   guess=male     name=Berry                         \n",
      "correct=female   guess=male     name=Bill                          \n",
      "correct=female   guess=male     name=Birgit                        \n",
      "correct=female   guess=male     name=Brandais                      \n",
      "correct=female   guess=male     name=Brandy                        \n",
      "correct=female   guess=male     name=Bridgett                      \n",
      "correct=female   guess=male     name=Britney                       \n",
      "correct=female   guess=male     name=Charmion                      \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (tag, guess, name) in sorted(errors)[:20]:\n",
    "    print('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Error Count - DT</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error count:  187\n"
     ]
    }
   ],
   "source": [
    "dt_errors = len(errors2)\n",
    "print(\"Error count: \", dt_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comparing Naive Bayes vs Decision Tree </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotdata (color, color1, label1, label2, setname, nb, dt, nb1, dt1, error= False):\n",
    "    if(error ) :\n",
    "        n_groups = 1\n",
    "    else :\n",
    "        n_groups = 4\n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.25\n",
    "    opacity = 0.75\n",
    "\n",
    "    rects1 = plt.bar(index, nb, bar_width,\n",
    "    alpha=opacity,\n",
    "    color=color,\n",
    "    label=label1)\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, nb1, bar_width,\n",
    "    alpha=opacity,\n",
    "    color='Y',\n",
    "    label='Feature Set 2')\n",
    "\n",
    "    rects2 = plt.bar(index + 2 + bar_width, dt, bar_width,\n",
    "    alpha=opacity,\n",
    "    color=color1,\n",
    "    label=label2)\n",
    "    \n",
    "    rects2 = plt.bar( index +3  + bar_width, dt1, bar_width,\n",
    "    alpha=opacity,\n",
    "    color='B',\n",
    "    label='Feature Set 2')\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title( setname + ' Set - NaiveBayes Vs DecisionTree')\n",
    "    if(not error):\n",
    "        plt.xticks(index + bar_width /4, ('Accuracy', 'Precision', 'Recall', 'F_Measure'))\n",
    " \n",
    "    #plt.xticks(index + bar_width / 4)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>\n",
    "\n",
    "<h4>How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?</h4>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naiv Features set 1 : \n",
      "       Accuracy  Precision  Recall  F_Measure\n",
      "Train  0.821721   0.446117     0.5   0.466987\n",
      "Test   0.802500   0.421146     0.5   0.453050\n",
      "Dev    0.802500   0.456967     0.5   0.473553\n",
      "Naive Features set 2: \n",
      "       Accuracy  Precision  Recall  F_Measure\n",
      "Train  0.779792   0.462746     0.5   0.468246\n",
      "Test   0.765000   0.438399     0.5   0.454943\n",
      "Dev    0.772500   0.473111     0.5   0.476667\n",
      "DT Features set 1 : \n",
      "       Accuracy  Precision  Recall  F_Measure\n",
      "Train  0.918821   0.494332     0.5   0.492855\n",
      "Test   0.770000   0.430172     0.5   0.453580\n",
      "Dev    0.766250   0.471696     0.5   0.475605\n",
      "DT Features set 2 : \n",
      "       Accuracy  Precision  Recall  F_Measure\n",
      "Train  0.803279   0.466522     0.5   0.472399\n",
      "Test   0.767500   0.435571     0.5   0.454684\n",
      "Dev    0.767500   0.468688     0.5   0.474969\n"
     ]
    }
   ],
   "source": [
    " \n",
    "print('Naiv Features set 1 : ')\n",
    "print(df_naive)\n",
    "print('Naive Features set 2: ')\n",
    "print(df_naive1)\n",
    "\n",
    "print('DT Features set 1 : ')\n",
    "print(df_dt)\n",
    "print('DT Features set 2 : ')\n",
    "print(df_dt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree model has better accuracy on Training and Test set.  Naive Bayes is slightly better on the  Dev Test set. Error rate  for Decision tree model is a lot better than Naive Bayes. Based on these I think Decision Tree model is a better model. Also, creating multiple features sets and analysing their performance on different models will give us better insight. Accuracy of feature set 1 is better than featuure set 2 in all cases, regardless of the classifier used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
