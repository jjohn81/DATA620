{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DATA 620 - Project 3</h2>\n",
    "<h3>Name Gender Classifier</h3>\n",
    "\n",
    "<h3>Joby John</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment</h3>\n",
    "\n",
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can.\n",
    "\n",
    "We will split dataset into three subsets. 80% data for training, 10 % data for dev-test and 10 %  for testing. We are mainting to the seperate test and dev-test. \n",
    "\n",
    "We will compare the performance of different classifiers on these datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk.classify import apply_features\n",
    "import collections\n",
    "import nltk.metrics\n",
    "from nltk.metrics import precision,recall,f_measure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data</h3>\n",
    "\n",
    "We will import the male and female ness from NLTk librarby and shuffle the names so the sampling is random acorss all the names.  'male.txt' contains 2943 names and 'female.txt' contains 5001 names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2943\n",
      "5001\n"
     ]
    }
   ],
   "source": [
    "random.seed(100)\n",
    "\n",
    "print(len(names.words('male.txt')))\n",
    "print(len(names.words('female.txt')))\n",
    "      \n",
    "names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "[(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "# and to make sure we are sampling across all the names\n",
    "# we shuffle them so they aren't alphabetical\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sample Data</h4>\n",
    "Belows we print some of the names in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pembroke', 'male'), ('Bryant', 'male'), ('Pier', 'female'), ('Florice', 'female'), ('Clint', 'male'), ('Ruthanne', 'female'), ('Upton', 'male'), ('Yetty', 'female'), ('Silvie', 'female'), ('Lizzie', 'female')]\n"
     ]
    }
   ],
   "source": [
    "#list names\n",
    " \n",
    "print(names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Split dataset : Train, Dev-Test and Test </h4>\n",
    "We will 10%(800 names) data fort Test and Dev-Test. 80%(6344) the data will be used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Names :  7944\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Names : \" , len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Count :  800\n",
      "dev test set :  800\n",
      "train set:  6344\n"
     ]
    }
   ],
   "source": [
    "test_set = names[:800]\n",
    "print(\"Test Set Count : \" , len(test_set))\n",
    "devtest_set = names[800:1600] \n",
    "print(\"dev test set : \" , len(devtest_set))\n",
    "train_set = names[1600:]\n",
    "print(\"train set: \" , len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Set - Gender</h3>\n",
    "We will define the features set as the first letter, last letter, last two letters, first two letters, number of vowles and has letter in the vowel. \n",
    "We will use Navie Bayes and Decision Trees and compare their performance to see which is good at predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(name):\n",
    "    f = {}\n",
    "    f[\"first\"] = name[0].lower()\n",
    "    f[\"last\"] = name[-1].lower()\n",
    "    f[\"suffix2\"]= name[-2:].lower()\n",
    "    f[\"preffix2\"]= name[:2].lower()\n",
    "    for letter in 'aeiou':\n",
    "        f[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        f[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(features(n), g) for (n,g) in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = featuresets[1000:]\n",
    "test_f = featuresets[:500]\n",
    "devtest_f =featuresets[500:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Naive Bayes </h3>\n",
    "We will use Naive Bayes classifier and calculate Accuracy, Recall, Precision and F_Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nbclassifier = nltk.NaiveBayesClassifier.train(train_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate Metrics: Accuracy, Recall, Precision, F_Measure </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(cls, dataset ):\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "\n",
    "    for i, (f, label) in enumerate(dataset):\n",
    "        refsets[label].add(i)\n",
    "        observed = cls.classify(f)\n",
    "        testsets[observed].add(i)\n",
    "\n",
    "    a = nltk.classify.accuracy(cls, dataset)\n",
    "    p = (precision(refsets['male'], testsets['male'])+precision(refsets['male'], testsets['female']))/2\n",
    "    r = (recall(refsets['male'], testsets['male'])+recall(refsets['male'], testsets['female']))/2\n",
    "    f = (f_measure(refsets['male'], testsets['male'])+f_measure(refsets['male'], testsets['female']))/2\n",
    "\n",
    "    return (a, p,r,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracy_train, avg_precision_train, avg_recall_train, avg_f_measure_train = getMetrics(nbclassifier, train_f)\n",
    " \n",
    "accuracy_test ,  avg_precision_test ,avg_recall_test , avg_f_measure_test = getMetrics(nbclassifier, test_f)\n",
    "\n",
    "accuracy_dev ,  avg_precision_dev ,avg_recall_dev , avg_f_measure_dev = getMetrics(nbclassifier, devtest_f)\n",
    "\n",
    " \n",
    "accuracy_naive_final = [accuracy_train,accuracy_test,accuracy_dev]\n",
    "precision_naive_final = [avg_precision_train,avg_precision_test,avg_precision_dev]\n",
    "recall_naive_final = [avg_recall_train,avg_recall_test,avg_recall_dev]\n",
    "f_measure_naive_final = [avg_f_measure_train,avg_f_measure_test,avg_f_measure_dev]\n",
    "\n",
    "naive_final_train = [accuracy_train,avg_precision_train,avg_recall_train,avg_f_measure_train]\n",
    "naive_final_test = [accuracy_test,avg_precision_test,avg_recall_test,avg_f_measure_test]\n",
    "naive_final_dev = [accuracy_dev,avg_precision_dev,avg_recall_dev,avg_f_measure_dev]\n",
    "naive_combined = {'Accuracy':accuracy_naive_final,'Precision':precision_naive_final,'Recall':recall_naive_final,'F_Measure':f_measure_naive_final}\n",
    "df_naive = pd.DataFrame(naive_combined,index=['Train','Test','Dev'],columns=['Accuracy','Precision','Recall','F_Measure'])\n",
    "print(df_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Naive Baiyer features </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nbclassifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate erorr rate</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check errors\n",
    "errors = []\n",
    "for (name, tag) in devtest_set:\n",
    "    guess = nbclassifier.classify(features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (tag, guess, name) in sorted(errors)[:20]:\n",
    "    print('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Error Count - NB</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "naive_errors = len(errors)\n",
    "print(\"Error count: \", naive_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decision Tree</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Decision Tree classifier and calculate Accuracy, Recall, Precision and F_Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtclassifier = nltk.DecisionTreeClassifier.train(train_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate Metrics</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train_dt, avg_precision_train_dt, avg_recall_train_dt , avg_f_measure_train_dt = getMetrics(dtclassifier, train_f)\n",
    " \n",
    "accuracy_test_dt, avg_precision_test_dt, avg_recall_test_dt , avg_f_measure_test_dt = getMetrics(dtclassifier, test_f)\n",
    "\n",
    "accuracy_dev_dt, avg_precision_dev_dt, avg_recall_dev_dt , avg_f_measure_dev_dt = getMetrics(dtclassifier, devtest_f)\n",
    "\n",
    " \n",
    "accuracy_dt_final = [accuracy_train_dt,accuracy_test_dt,accuracy_dev_dt]\n",
    "precision_dt_final = [avg_precision_train_dt,avg_precision_test_dt,avg_precision_dev_dt]\n",
    "recall_dt_final = [avg_recall_train_dt,avg_recall_test_dt,avg_recall_dev_dt]\n",
    "f_measure_dt_final = [avg_f_measure_train_dt,avg_f_measure_test_dt,avg_f_measure_dev_dt]\n",
    "\n",
    "dt_final_train = [accuracy_train_dt,avg_precision_train_dt,avg_recall_train_dt,avg_f_measure_train_dt]\n",
    "dt_final_test = [accuracy_test_dt,avg_precision_test_dt,avg_recall_test_dt,avg_f_measure_test_dt]\n",
    "dt_final_dev = [accuracy_dev_dt,avg_precision_dev_dt,avg_recall_dev_dt,avg_f_measure_dev_dt]\n",
    "dt_combined = {'Accuracy':accuracy_dt_final,'Precision':precision_dt_final,'Recall':recall_dt_final,'F_Measure':f_measure_dt_final}\n",
    "df_dt = pd.DataFrame(dt_combined,index=['Train','Test','Dev'],columns=['Accuracy','Precision','Recall','F_Measure'])\n",
    "print(df_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Calculate error rate </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors2 = []\n",
    "for (name, tag) in devtest_set:\n",
    "    guess = dtclassifier.classify(features(name))\n",
    "    if guess != tag:\n",
    "        errors2.append( (tag, guess, name) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (tag, guess, name) in sorted(errors)[:20]:\n",
    "    print('correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Error Count - DT</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_errors = len(errors2)\n",
    "print(\"Error count: \", dt_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comparing Naive Bayes vs Decision Tree </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotdata (color, color1, label1, label2, setname, nb, dt, error= False):\n",
    "    if(error ) :\n",
    "        n_groups = 1\n",
    "    else :\n",
    "        n_groups = 4 \n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.25\n",
    "    opacity = 0.75\n",
    "\n",
    "    rects1 = plt.bar(index, nb, bar_width,\n",
    "    alpha=opacity,\n",
    "    color=color,\n",
    "    label=label1)\n",
    "\n",
    "    rects2 = plt.bar(index + bar_width, dt, bar_width,\n",
    "    alpha=opacity,\n",
    "    color=color1,\n",
    "    label=label2)\n",
    "\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title( setname + ' Set - NaiveBayes Vs DecisionTree')\n",
    "    if(not error):\n",
    "        plt.xticks(index + bar_width, ('Accuracy', 'Precision', 'Recall', 'F_Measure'))\n",
    " \n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>\n",
    "\n",
    "<h4>How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?</h4>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotdata('r','g','NaiveBayes','DecisionTree', 'Training',naive_final_train,dt_final_train,False)\n",
    "\n",
    "plotdata('r','g','NaiveBayes','DecisionTree', 'Test',naive_final_test,dt_final_test,False)\n",
    "\n",
    "plotdata('r','g','NaiveBayes','DecisionTree', 'DevTest',naive_final_dev,dt_final_dev,False)\n",
    " \n",
    "plotdata('r','g','NaiveBayes','DecisionTree', 'Error',naive_errors,dt_errors, True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree model has better accuracy on Training and Test set.  Naive Bayes is slightly better on the  Dev Test set. Error rate  for Decision tree model is a lot better than Naive Bayes. Based on these I think Decision Tree model is a better model. Also, creating multiple features sets and analysing their performance on different models will give us better insight. That was something I wanted to do but due to the time constraints I was not able to do that. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
